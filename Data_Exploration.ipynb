{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing libraries for data loading and visualization\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading reddit may 2015 comment data from kaggle sql database\n",
    "\n",
    "sql_conn = sqlite3.connect(\"data/database.sqlite\")\n",
    "\n",
    "all_comments = pd.read_sql(\"SELECT author, subreddit, body FROM May2015\", sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating new column for comment length\n",
    "\n",
    "comment_length = []\n",
    "\n",
    "for row in all_comments['body']:\n",
    "    if pd.isnull(row):\n",
    "        comment_length.append(0)\n",
    "    else:\n",
    "        comment_length.append(len(row))\n",
    "    \n",
    "all_comments['comment_length'] = comment_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total comments: \", all_comments.shape[0])\n",
    "print(\"Total subreddits: \", all_comments['subreddit'].nunique())\n",
    "print(\"Unique authors: \", all_comments['author'].nunique())\n",
    "print(\"Average comment length: \", all_comments['comment_length'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating subset of comments for r/nfl\n",
    "\n",
    "sub = ['nfl']\n",
    "\n",
    "nfl_comments = all_comments.loc[all_comments['subreddit'].isin(sub)]\n",
    "\n",
    "print(\"Total r/nfl comments: \", nfl_comments.shape[0])\n",
    "print(\"Unique r/nfl authors: \", nfl_comments['author'].nunique())\n",
    "print(\"Average r/nfl comment length: \", nfl_comments['comment_length'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying the top 10 authors for modeling, excluding deleted/mod/bot accounts\n",
    "\n",
    "exclusion_list = [\n",
    "    '[deleted]',\n",
    "    'AutoModerator'\n",
    "]\n",
    "\n",
    "nfl_top_10_authors = nfl_comments['author'].loc[~nfl_comments['author'].isin(exclusion_list)].value_counts()[:10].index.values\n",
    "\n",
    "print(\"Top 10 authors: \", nfl_top_10_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at average comment length and number of comments by author\n",
    "\n",
    "nfl_comments_top_10_all = nfl_comments.loc[nfl_comments['author'].isin(nfl_top_10_authors)].drop_duplicates(subset = [\"author\", \"body\"])\n",
    "\n",
    "nfl_top_comments_length = nfl_comments_top_10_all.groupby('author', as_index=False)['comment_length'].mean().sort_values('comment_length', ascending = False)\n",
    "nfl_top_comments_count = nfl_comments_top_10_all['author'].value_counts()\n",
    "\n",
    "plt.figure()\n",
    "nfl_top_comments_length.plot.bar(x = 'author', y = 'comment_length', figsize=(20,10))\n",
    "plt.figure()\n",
    "nfl_top_comments_count.plot.bar(x = 'author', y = 'comments', figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sampling comments to get an even number for each author to avoid bias\n",
    "\n",
    "nfl_comments_top_10_sampled = nfl_comments_top_10_all.groupby('author', group_keys=False).apply(lambda nfl_comments_top_10_all: nfl_comments_top_10_all.sample(1000))\n",
    "nfl_comments_top_10_sampled.to_csv('data/nfl_comments_top_10_sampled.csv', columns = ['author','body'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating subset of comments for r/nba\n",
    "\n",
    "sub = ['nba']\n",
    "\n",
    "nba_comments = all_comments.loc[all_comments['subreddit'].isin(sub)]\n",
    "\n",
    "print(\"Total r/nba comments: \", nba_comments.shape[0])\n",
    "print(\"Unique r/nba authors: \", nba_comments['author'].nunique())\n",
    "print(\"Average r/nba comment length: \", nba_comments['comment_length'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying the top 10 authors for modeling, excluding deleted/mod/bot accounts\n",
    "\n",
    "exclusion_list = [\n",
    "    '[deleted]',\n",
    "    'AutoModerator'\n",
    "]\n",
    "\n",
    "nba_top_10_authors = nba_comments['author'].loc[~nba_comments['author'].isin(exclusion_list)].value_counts()[:10].index.values\n",
    "\n",
    "print(\"Top 10 authors: \", nba_top_10_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at average comment length and number of comments by author\n",
    "\n",
    "nba_comments_top_10_all = nba_comments.loc[nba_comments['author'].isin(nba_top_10_authors)].drop_duplicates(subset = [\"author\", \"body\"])\n",
    "\n",
    "print(np.shape(nba_comments_top_10_all))\n",
    "\n",
    "nba_comments_top_10_all = nba_comments_top_10_all.dropna(axis=1, how='any')\n",
    "\n",
    "print(np.shape(nba_comments_top_10_all))\n",
    "\n",
    "nba_top_comments_length = nba_comments_top_10_all.groupby('author', as_index=False)['comment_length'].mean().sort_values('comment_length', ascending = False)\n",
    "nba_top_comments_count = nba_comments_top_10_all['author'].value_counts()\n",
    "\n",
    "plt.figure()\n",
    "nba_top_comments_length.plot.bar(x = 'author', y = 'comment_length', figsize=(20,10))\n",
    "plt.figure()\n",
    "nba_top_comments_count.plot.bar(x = 'author', y = 'comments', figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sampling comments to get an even number for each author to avoid bias\n",
    "\n",
    "nba_comments_top_10_sampled = nba_comments_top_10_all.groupby('author', group_keys=False).apply(lambda nba_comments_top_10_all: nba_comments_top_10_all.sample(1300))\n",
    "nba_comments_top_10_sampled.to_csv('data/nba_comments_top_10_sampled.csv', columns = ['author','body'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
